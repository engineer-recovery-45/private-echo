<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>PrivateEcho - AI Voice Diary</title>

<style>
body {
  font-family: -apple-system, BlinkMacSystemFont, sans-serif;
  background: #111;
  color: white;
  text-align: center;
  padding: 40px;
}

button {
  padding: 15px 30px;
  font-size: 18px;
  border: none;
  border-radius: 10px;
  margin: 10px;
  cursor: pointer;
}

#record { background: #e63946; color: white; }
#stop { background: #457b9d; color: white; }

textarea {
  width: 90%;
  max-width: 600px;
  height: 200px;
  margin-top: 30px;
  padding: 15px;
  border-radius: 10px;
  border: none;
  font-size: 16px;
}

#status {
  margin-top: 20px;
  font-size: 14px;
  opacity: 0.7;
}
</style>
</head>
<body>

<h1>ğŸ¤ PrivateEcho</h1>
<p>AI Voice Diaryï¼ˆé«˜ç²¾åº¦ç‰ˆï¼‰</p>

<button id="record">éŒ²éŸ³é–‹å§‹</button>
<button id="stop" disabled>åœæ­¢</button>

<div id="status">ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰</div>
<textarea id="output" placeholder="ã“ã“ã«æ–‡å­—èµ·ã“ã—çµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™"></textarea>

<script type="module">
import { pipeline } from "https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2";

let recorder;
let chunks = [];
let transcriber;
let isModelLoading = false;

const recordBtn = document.getElementById("record");
const stopBtn = document.getElementById("stop");
const output = document.getElementById("output");
const statusText = document.getElementById("status");

async function loadModel() {
  if (transcriber || isModelLoading) return;
  isModelLoading = true;
  statusText.textContent = "ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­ï¼ˆåˆå›ã¯1ã€œ2åˆ†ï¼‰...";

  transcriber = await pipeline(
    "automatic-speech-recognition",
    "Xenova/whisper-small",
    {
      quantized: true,
      chunk_length_s: 30,
      stride_length_s: 5
    }
  );

  statusText.textContent = "ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†";
}

recordBtn.onclick = async () => {
  await loadModel();

  const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
  recorder = new MediaRecorder(stream);
  chunks = [];

  recorder.ondataavailable = e => chunks.push(e.data);

  recorder.onstop = async () => {
    statusText.textContent = "éŸ³å£°å‡¦ç†ä¸­...";

    const blob = new Blob(chunks, { type: recorder.mimeType || "audio/webm" });
    const arrayBuffer = await blob.arrayBuffer();

    const audioContext = new (window.AudioContext || window.webkitAudioContext)({
      sampleRate: 16000
    });

    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

    // ãƒ¢ãƒãƒ©ãƒ«å–å¾—
    let channelData = audioBuffer.getChannelData(0);

    // æ­£è¦åŒ–
    let max = 0;
    for (let i = 0; i < channelData.length; i++) {
      max = Math.max(max, Math.abs(channelData[i]));
    }
    if (max > 0) {
      for (let i = 0; i < channelData.length; i++) {
        channelData[i] /= max;
      }
    }

    // ç„¡éŸ³ãƒˆãƒªãƒŸãƒ³ã‚°
    const threshold = 0.01;
    let start = 0;
    let end = channelData.length - 1;

    while (start < channelData.length && Math.abs(channelData[start]) < threshold) start++;
    while (end > start && Math.abs(channelData[end]) < threshold) end--;

    const trimmed = channelData.slice(start, end);

    statusText.textContent = "æ–‡å­—èµ·ã“ã—ä¸­...";

    const result = await transcriber(trimmed, {
      language: "japanese",
      task: "transcribe",
      temperature: 0.0,
      beam_size: 5,
      best_of: 5
    });

    output.value = result.text;
    statusText.textContent = "å®Œäº†";
  };

  recorder.start();
  recordBtn.disabled = true;
  stopBtn.disabled = false;
  statusText.textContent = "éŒ²éŸ³ä¸­...";
};

stopBtn.onclick = () => {
  recorder.stop();
  recordBtn.disabled = false;
  stopBtn.disabled = true;
};

</script>
</body>
</html>
